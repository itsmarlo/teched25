apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  name: invoice-classifier-training
  annotations:
    scenarios.ai.sap.com/description: "Train invoice classifier model"
    scenarios.ai.sap.com/name: "invoice-classification"
    executables.ai.sap.com/description: "Training pipeline for invoice classification"
    executables.ai.sap.com/name: "training"
    artifacts.ai.sap.com/invoicesdataset.kind: "dataset"
    artifacts.ai.sap.com/invoiceclassifiermodel.kind: "model"
    artifacts.ai.sap.com/evalmetrics.kind: "metrics"
  labels:
    scenarios.ai.sap.com/id: "invoice-classification"
    ai.sap.com/version: "1.0.0"
spec:
  imagePullSecrets:
    - name: docker-registry-secret
  entrypoint: training-pipeline
  arguments:
    parameters:
      # Training image in your registry
      - name: trainImage
        value: "docker.io/itsmarlo/invoice-train:latest"
      # Hyperparams below are placeholders; current train.py ignores them.
      - name: baseModel
        value: "microsoft/deberta-v3-base"
      - name: numEpochs
        value: "3"
      - name: learningRate
        value: "2e-5"
      - name: batchSize
        value: "16"
      # Used by train.py
      - name: numLabels
        value: "5"
  templates:
    - name: training-pipeline
      inputs:
        artifacts:
          - name: invoicesdataset
            path: /app/data/
      steps:
        - - name: train-model
            template: model-training
            arguments:
              artifacts:
                - name: invoicesdataset
                  from: "{{inputs.artifacts.invoicesdataset}}"
        - - name: evaluate-model
            template: model-evaluation
            arguments:
              artifacts:
                - name: trainedModel
                  from: "{{steps.train-model.outputs.artifacts.trainedModel}}"
        - - name: save-model
            template: model-storage
            arguments:
              artifacts:
                - name: trainedModel
                  from: "{{steps.train-model.outputs.artifacts.trainedModel}}"
                - name: evalmetrics
                  from: "{{steps.evaluate-model.outputs.artifacts.evalmetrics}}"

    # === Step 1: Train ===
    - name: model-training
      metadata:
        labels:
          ai.sap.com/resourcePlan: train.l
      inputs:
        artifacts:
          - name: invoicesdataset
            path: /app/data/
      outputs:
        artifacts:
          - name: trainedModel
            globalName: invoiceclassifiermodel
            path: /app/invoice_classifier/
            archive: { none: {} }
      container:
        image: "{{workflow.parameters.trainImage}}"
        imagePullPolicy: IfNotPresent
        command: ["/bin/sh","-lc"]
        args:
          - |
            set -e
            mkdir -p /app/invoice_classifier
            # Env that train.py actually reads
            export OUTPUT_DIR=/app/invoice_classifier
            export NUM_LABELS="{{workflow.parameters.numLabels}}"
            # (Optional future wiringâ€”currently unused by your train.py)
            export MODEL_NAME="{{workflow.parameters.baseModel}}"
            export NUM_EPOCHS="{{workflow.parameters.numEpochs}}"
            export LEARNING_RATE="{{workflow.parameters.learningRate}}"
            export BATCH_SIZE="{{workflow.parameters.batchSize}}"
            # Run training (dataset already mounted at /app/data/)
            python /app/train.py

    # === Step 2: Evaluate (placeholder) ===
    - name: model-evaluation
      metadata:
        labels:
          ai.sap.com/resourcePlan: train.l
      inputs:
        artifacts:
          - name: trainedModel
            path: /app/invoice_classifier/
      outputs:
        artifacts:
          - name: evalmetrics
            globalName: evalmetrics
            path: /app/output/metrics/
            archive: { none: {} }
      container:
        image: "{{workflow.parameters.trainImage}}"
        imagePullPolicy: IfNotPresent
        command: ["python","-c"]
        args:
          - |
            import json, os
            os.makedirs("/app/output/metrics/", exist_ok=True)
            # TODO: replace with real evaluation using your val set
            with open("/app/output/metrics/metrics.json","w") as f:
              json.dump({"status":"ok","note":"replace with real evaluation"}, f)

    # === Step 3: Persist artifacts (AI Core collects these) ===
    - name: model-storage
      metadata:
        labels:
          ai.sap.com/resourcePlan: train.s
      inputs:
        artifacts:
          - name: trainedModel
            path: /app/final/model/
          - name: evalmetrics
            path: /app/final/metrics/
      container:
        image: alpine:3.19
        command: ["/bin/sh","-lc"]
        args: ["echo 'Artifacts staged at /app/final; AI Core will upload them.'"]

