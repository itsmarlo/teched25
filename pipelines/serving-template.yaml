apiVersion: ai.sap.com/v1alpha1
kind: ServingTemplate
metadata:
  name: invoice-classifier-serving
  annotations:
    scenarios.ai.sap.com/name: "invoice-classification"
    scenarios.ai.sap.com/description: "Blocked invoice classification using DeBERTa-v3-base"
    executables.ai.sap.com/name: "invoice-classifier"
    executables.ai.sap.com/description: "Classify blocked invoices in SAP S/4HANA"
    artifacts.ai.sap.com/invoiceclassifiermodel.kind: "model"
  labels:
    scenarios.ai.sap.com/id: "invoice-classification"
    ai.sap.com/version: "1.0.0"
spec:
  inputs:
    artifacts:
      - name: invoiceclassifiermodel          # comes from your training workflow (globalName)
    parameters:
      - name: serviceName
        type: string
        default: "invoice-classifier"
      - name: image                           # <â€” full image incl. repo
        type: string
        default: "docker.io/yourrepo/invoice-serve:latest"
      - name: imagePullSecret
        type: string
        default: "docker-registry-secret"
      - name: port
        type: integer
        default: 8080
      - name: minReplicas
        type: integer
        default: 1
      - name: maxReplicas
        type: integer
        default: 3
      - name: resourcePlan
        type: string
        default: "infer.s"                    # choose infer.m / infer.l or GPU plan if needed
  template:
    apiVersion: "serving.kserve.io/v1beta1"
    kind: InferenceService
    metadata:
      name: "{{.inputs.parameters.serviceName}}"
      annotations: |
        autoscaling.knative.dev/metric: concurrency
        autoscaling.knative.dev/target: "1"
        autoscaling.knative.dev/targetBurstCapacity: "0"
      labels: |
        ai.sap.com/resourcePlan: {{.inputs.parameters.resourcePlan}}
    spec: |
      predictor:
        imagePullSecrets:
          - name: {{.inputs.parameters.imagePullSecret}}
        minReplicas: {{.inputs.parameters.minReplicas}}
        maxReplicas: {{.inputs.parameters.maxReplicas}}

        # KServe will download the model (from STORAGE_URI) into the volume named
        # 'kserve-provision-location'. Your container reads it from /mnt/models.
        containers:
        - name: kserve-contai

